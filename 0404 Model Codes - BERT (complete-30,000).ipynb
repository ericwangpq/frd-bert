{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b59ca9f5",
   "metadata": {},
   "source": [
    "# Load data from YelpZip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c1ddb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b41496b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metad = pd.read_csv('metadata', delimiter='\\t', names=['user_id','prod_id','rating','label','date'], index_col = None)\n",
    "review = pd.read_csv('reviewContent', delimiter='\\t', names=['user_id','prod_id','date','review'], index_col = None)\n",
    "\n",
    "data = pd.merge(metad, review, on=['user_id','prod_id','date'], how='inner')\n",
    "\n",
    "label_mapping = {-1: 0, 1: 1} #0 for fake, 1 for genuine\n",
    "data['label'] = data['label'].map(label_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d83c959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>prod_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5044</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-11-16</td>\n",
       "      <td>Drinks were bad, the hot chocolate was watered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5045</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-08</td>\n",
       "      <td>This was the worst experience I've ever had a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5046</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-10-06</td>\n",
       "      <td>This is located on the site of the old Spruce ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5047</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-11-30</td>\n",
       "      <td>I enjoyed coffee and breakfast twice at Toast ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5048</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-08-28</td>\n",
       "      <td>I love Toast! The food choices are fantastic -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608453</th>\n",
       "      <td>119664</td>\n",
       "      <td>5039</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-20</td>\n",
       "      <td>When I first moved to the area I must say I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608454</th>\n",
       "      <td>56277</td>\n",
       "      <td>5039</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-12</td>\n",
       "      <td>Kind of pricey. I guess I expected a ridiculou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608455</th>\n",
       "      <td>265320</td>\n",
       "      <td>5039</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-08-22</td>\n",
       "      <td>Stopped by this restaurant yesterday, we just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608456</th>\n",
       "      <td>161722</td>\n",
       "      <td>5039</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-05-11</td>\n",
       "      <td>Finally checked out The Best Subs in Claremont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608457</th>\n",
       "      <td>78454</td>\n",
       "      <td>5039</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-17</td>\n",
       "      <td>Just got me some \"Best Subs\" and I gotta say, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>608458 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  prod_id  rating  label        date  \\\n",
       "0          5044        0     1.0      0  2014-11-16   \n",
       "1          5045        0     1.0      0  2014-09-08   \n",
       "2          5046        0     3.0      0  2013-10-06   \n",
       "3          5047        0     5.0      0  2014-11-30   \n",
       "4          5048        0     5.0      0  2014-08-28   \n",
       "...         ...      ...     ...    ...         ...   \n",
       "608453   119664     5039     4.0      1  2013-01-20   \n",
       "608454    56277     5039     2.0      1  2012-11-12   \n",
       "608455   265320     5039     1.0      1  2012-08-22   \n",
       "608456   161722     5039     4.0      1  2011-05-11   \n",
       "608457    78454     5039     4.0      1  2010-07-17   \n",
       "\n",
       "                                                   review  \n",
       "0       Drinks were bad, the hot chocolate was watered...  \n",
       "1       This was the worst experience I've ever had a ...  \n",
       "2       This is located on the site of the old Spruce ...  \n",
       "3       I enjoyed coffee and breakfast twice at Toast ...  \n",
       "4       I love Toast! The food choices are fantastic -...  \n",
       "...                                                   ...  \n",
       "608453  When I first moved to the area I must say I wa...  \n",
       "608454  Kind of pricey. I guess I expected a ridiculou...  \n",
       "608455  Stopped by this restaurant yesterday, we just ...  \n",
       "608456  Finally checked out The Best Subs in Claremont...  \n",
       "608457  Just got me some \"Best Subs\" and I gotta say, ...  \n",
       "\n",
       "[608458 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7b2087d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    528019\n",
      "0     80439\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_counts = data['label'].value_counts()\n",
    "\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b87f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d143b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for training\n",
    "label_counts = data['label'].value_counts()\n",
    "\n",
    "# 计算各标签应该采样的数量\n",
    "sample_size_true = int(sample_size / 2)\n",
    "sample_size_false =int(sample_size / 2)\n",
    "\n",
    "\n",
    "# 按标签分组并进行分层抽样\n",
    "stratified_sample = data.groupby('label').apply(lambda x: x.sample(n=sample_size_true if x.name == 1 else sample_size_false))\n",
    "\n",
    "# 重置索引\n",
    "stratified_sample = stratified_sample.reset_index(drop=True)\n",
    "df_selected = stratified_sample.copy()\n",
    "\n",
    "# 找出未被选中的数据\n",
    "data['selected'] = data.index.isin(df_selected.index)\n",
    "df_unselected = data[data['selected'] == False].drop(columns='selected')\n",
    "\n",
    "data = df_selected\n",
    "# Here, df_selected contains the data selected and put into the datafile, while df_unselected contains the remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5727fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_test for testing\n",
    "label_counts = df_unselected['label'].value_counts()\n",
    "\n",
    "# 计算各标签应该采样的数量\n",
    "sample_size = sample_size * 0.2\n",
    "sample_size_true = int(sample_size / 2)\n",
    "sample_size_false =int(sample_size / 2)\n",
    "\n",
    "\n",
    "# 按标签分组并进行分层抽样\n",
    "stratified_sample = df_unselected.groupby('label').apply(lambda x: x.sample(n=sample_size_true if x.name == 1 else sample_size_false))\n",
    "\n",
    "# 重置索引\n",
    "stratified_sample = stratified_sample.reset_index(drop=True)\n",
    "df_selected = stratified_sample.copy()\n",
    "\n",
    "# 找出未被选中的数据\n",
    "data['selected'] = data.index.isin(df_selected.index)\n",
    "df_unselected = data[data['selected'] == False].drop(columns='selected')\n",
    "\n",
    "data_test = df_selected\n",
    "# Here, df_selected contains the data selected and put into the datafile, while df_unselected contains the remaining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a619669",
   "metadata": {},
   "source": [
    "# BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6550866",
   "metadata": {},
   "source": [
    "1. Load necceary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6fb0af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfd1d5e",
   "metadata": {},
   "source": [
    "2. Read and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32b22c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeReviewDataset(Dataset):\n",
    "\n",
    "    def __init__(self, reviews, labels, tokenizer, max_length):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = str(self.reviews[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'review_text': review,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_length = 256\n",
    "\n",
    "df_train, df_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=42)\n",
    "\n",
    "train_dataset = FakeReviewDataset(df_train['review'].to_numpy(), df_train['label'].to_numpy(), tokenizer, max_length)\n",
    "val_dataset = FakeReviewDataset(df_val['review'].to_numpy(), df_val['label'].to_numpy(), tokenizer, max_length)\n",
    "test_dataset = FakeReviewDataset(df_test['review'].to_numpy(), df_test['label'].to_numpy(), tokenizer, max_length)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a65c4",
   "metadata": {},
   "source": [
    "3. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03a5f43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac2d5658123449c8c44be6b7a91efde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "num_epochs = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416925c7",
   "metadata": {},
   "source": [
    "4. Build the helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "621e6f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_eval_model(model, dataloader, optimizer=None, is_training=True):\n",
    "    if is_training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_accuracy = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if is_training:\n",
    "            optimizer.zero_grad\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        total_accuracy += torch.sum(preds == labels)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_accuracy = total_accuracy / len(dataloader.dataset)\n",
    "\n",
    "    return avg_loss, avg_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769b358f",
   "metadata": {},
   "source": [
    "5. Test and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4847d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 21/250 [11:29<2:05:16, 32.82s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b1bdd2214130>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_eval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-959b40e0bc35>\u001b[0m in \u001b[0;36mtrain_eval_model\u001b[0;34m(model, dataloader, optimizer, is_training)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1560\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1562\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1563\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         )\n\u001b[0;32m-> 1020\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1021\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m                 )\n\u001b[1;32m    609\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    611\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 425\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_loss, train_accuracy = train_eval_model(model, train_dataloader, optimizer, is_training=True)\n",
    "    print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")\n",
    "\n",
    "    val_loss, val_accuracy = train_eval_model(model, val_dataloader, is_training=False)\n",
    "    print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b05b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = train_eval_model(model, test_dataloader, is_training=False)\n",
    "print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15a223",
   "metadata": {},
   "source": [
    "6. Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f49aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"fake_review_detector\")\n",
    "tokenizer.save_pretrained(\"fake_review_detector\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099d765d",
   "metadata": {},
   "source": [
    "7. Reuse the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02498582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification, BertTokenizer, pipeline\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(\"fake_review_detector\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"fake_review_detector\")\n",
    "# review_detector = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7639dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Your specific text for classification goes here.\"\n",
    "# result = review_detector(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7031d61b",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746f96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b990b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertForSequenceClassification.from_pretrained(\"fake_review_detector\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"fake_review_detector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_reviews(reviews):\n",
    "    review_classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "    inputs = tokenizer(reviews, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    results = outputs.logits.detach().numpy()\n",
    "    return np.argmax(results, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test = data_test[\"review\"].tolist()\n",
    "labels_test = data_test[\"label\"].tolist()  # Assuming label 0 is \"FAKE\" and label 1 is \"REAL\"\n",
    "\n",
    "predicted_labels = detect_reviews(reviews_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eeb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(labels_test, predicted_labels)\n",
    "print(\"Confusion Matrix:\\n\", conf_mat)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(labels_test, predicted_labels, target_names=[\"FAKE\", \"REAL\"])\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n",
    "\n",
    "# Accuracy Score\n",
    "acc_score = accuracy_score(labels_test, predicted_labels)\n",
    "print(\"\\nAccuracy Score:\", acc_score)\n",
    "\n",
    "# Precision Score\n",
    "prec_score = precision_score(labels_test, predicted_labels)\n",
    "print(\"\\nPrecision Score:\", prec_score)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(labels_test, predicted_labels)\n",
    "print(\"\\nRecall:\", recall)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(labels_test, predicted_labels)\n",
    "print(\"\\nF1 Score:\", f1)\n",
    "\n",
    "# ROC AUC\n",
    "roc_auc = roc_auc_score(labels_test, predicted_labels)\n",
    "print(\"\\nROC AUC:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f83c8fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, _ = roc_curve(labels_test, predicted_labels)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f010bd",
   "metadata": {},
   "source": [
    "# OpenAI documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77bc1bb",
   "metadata": {},
   "source": [
    "GPT-3 fine tuning\n",
    "https://platform.openai.com/docs/guides/fine-tuning\n",
    "\n",
    "GPT-3 classification\n",
    "https://platform.openai.com/examples/default-classification\n",
    "\n",
    "GPT-3 model family\n",
    "https://platform.openai.com/docs/models/gpt-3\n",
    "\n",
    "GPT-2 based classification (deprecated)\n",
    "https://platform.openai.com/docs/guides/classifications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
